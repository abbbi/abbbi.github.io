---
layout: post
title: dd, bs= and why you should use conv=fsync
---

This story starts with me having to simulate a faulty disk device for testing.
The Linux Kernel Device mapper is a good solution for this, so i went on
creating a faulty device with a simple file backend:

`
 truncate -s 1G /tmp/baddisk
 losetup /dev/loop2 /tmp/baddisk
 dmsetup create baddisk << EOF
    0 6050 linear /dev/loop2 0
    6050 155 error
    6205 2090947 linear /dev/loop2 6205 
 EOF
`

These commands setup a new device on `/dev/mapper/baddisk` with 1 GB of size.
Starting from sector 6050, there are 155 faulty sectors, where any write and
read operation should cause I/O errors.

So i went on and used DD to write to the device, as the first faulty block
should start around 3 MB, i used the following command:

`
  dd if=/dev/zero of=/dev/mapper/baddisk bs=5M count=1
  5242880 bytes (5.2 MB, 5.0 MiB) copied, 0.0130177 s, 403 MB/s
`

To my surprise, the write operation succeeded. I assumed some error in my setup
and re-created the device mapper target, and tried again, this time with the
following command:

```
 dd if=/dev/zero of=/dev/mapper/baddisk
 dd: writing to '/dev/mapper/baddisk': Input/output error
 3096576 bytes (3.1 MB, 3.0 MiB) copied, 0.0238947 s, 130 MB/s
```

Nice, the device behaves as expected! So while taking notes in another terminal
and switching back and forth workspaces, i issued the following command again:

`
 dd if=/dev/zero of=/dev/mapper/baddisk bs=5M count=1
 5242880 bytes (5.2 MB, 5.0 MiB) copied, 0.0117504 s, 446 MB/s
`

What? It succeeded writing 5MB of data to a faulty segment of the disk which
should clearly fail! This was strange so i tried back and forth with this
command writing to the complete device until it got end of space, no I/O errors
were reported by dd.

Looking at the dmesg output, the kernel clearly noticed errors with the
underlying device:

`
 Buffer I/O error on dev dm-3, logical block 757, lost async page write
`

Wo why does dd not report this error?

The difference between the command is the used block size, so i assumed some
caching beeing the cause for this situation, or maybe dd opening the file with
other optins like O_DIRECT or O_SYNC if a bigger block size is used?

I straced both commands and the openat/write and close functions behaved
exacly the same:

`
 openat(AT_FDCWD, "/dev/mapper/baddisk", O_WRONLY|O_CREAT|O_TRUNC, 0666) = 3
 write(1, "\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0\0"..., 5242880) = 5242880
 close(1)                                = 0
`

The strace shows clearly that the succeeding command opens the file without any
notable difference to the command writing only with 512 bytes block size.  The
write and close functions return with no error whatsoever, so clearly, dd does
not notice it has just lost data while writing to the storage!

Making dd use the O_DIRECT flag during file open, or the O_SYNC option catches
the error:

`
 dd if=/dev/zero of=/dev/mapper/baddisk bs=5M oflag=direct
 dd: error writing '/dev/mapper/baddisk': Input/output error
`

So what is the reason for this? I assume dd, with its standard block size of
512 bytes does not the hit the linux kernels buffered I/O. And with bigger
block sizes, the I/O becomes buffered, async, and as it stands, it is a real
issue for user space applications on linux to notice write errors during
buffered I/O operations!

This leads us to my next finding, the linux fsync() gate that
dates back to 2018, starting with the following question on
stackoverflow:

 [https://stackoverflow.com/questions/42434872/writing-programs-to-cope-with-i-o-errors-causing-lost-writes-on-linux]
 
And resulting LWN articles:

 [https://lwn.net/Articles/724307/]
 
 [https://lwn.net/Articles/752063/]

which provide great insight into the linux kernels error handling and how these
errors are upstreamed to user space applications while writing data to faulty
devices.
 
Long story short: If one uses DS with a greater block size, be sure to use
either oflag=direct or conv=fsync to note any errors while writing data to a
device.
